{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14OXnwYeyZSS"
   },
   "source": [
    "# Bird Species Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z4wh8OIayhkn"
   },
   "source": [
    "In this project we will create a Convolutional Neural Network which will be able to predict species of the bird. We will use different layers and other hyperparameters for building, training and testing this multiclass classifictaion model. We will be using keras for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0UlFUS5fyNt2"
   },
   "outputs": [],
   "source": [
    "# Load the Drive helper and mount\n",
    "from google.colab import drive\n",
    "\n",
    "# This will prompt for authorization.\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aRGInah67Hjl"
   },
   "source": [
    "First we will mount our google drive on colab so that we can use the dataset directly from our drive. For this you first need to upload the data on your drive and then mount the drive on colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wu6mxNRIyQ3M"
   },
   "outputs": [],
   "source": [
    "# After executing the cell above, Drive files will be present in \"/content/drive/My Drive\".\n",
    "!ls \"/content/drive/My Drive\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HIZ61VM97OT-"
   },
   "source": [
    "After mounting our drive we will locate the folder where our data is stored to use it in our colab notebook. Here you can see that all the folders I have in my drive and 'Bird Species Dataset' contains the images that we will work on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5mJaMUYDyxoD"
   },
   "outputs": [],
   "source": [
    "# Library imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "import cv2\n",
    "import random\n",
    "from os import listdir\n",
    "from sklearn.preprocessing import  LabelBinarizer\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import img_to_array, array_to_img\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Activation, Flatten, Dropout, Dense\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xwjsH1qm7egp"
   },
   "source": [
    "We will start by importing some required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "duP0-XvFy1C9"
   },
   "outputs": [],
   "source": [
    "# Plotting 12 images to check dataset\n",
    "plt.figure(figsize=(12,12))\n",
    "path = \"/content/drive/My Drive/Bird Speciees Dataset/AMERICAN GOLDFINCH\"\n",
    "for i in range(1,17):\n",
    "    plt.subplot(4,4,i)\n",
    "    plt.tight_layout()\n",
    "    rand_img = imread(path +'/'+ random.choice(sorted(listdir(path))))\n",
    "    plt.imshow(rand_img)\n",
    "    plt.xlabel(rand_img.shape[1], fontsize = 10)\n",
    "    plt.ylabel(rand_img.shape[0], fontsize = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zxtlrISk7piy"
   },
   "source": [
    "Let's visualize some of the bird images that we will be working on. Also we will observe x and y dimensions of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0D8FRY23zH2t"
   },
   "outputs": [],
   "source": [
    "# Setting path and creating empty list\n",
    "dir = \"/content/drive/My Drive/Bird Speciees Dataset\"\n",
    "root_dir = listdir(dir)\n",
    "image_list, label_list = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JPjqjMI573DX"
   },
   "source": [
    "Setting the root directory for the dataset and storing all the floders name of the dataset. We will also create 2 empty list for image and lables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0qY4LwMBzvpx"
   },
   "outputs": [],
   "source": [
    "# Reading and converting image to numpy array\n",
    "for directory in root_dir:\n",
    "  for files in listdir(f\"{dir}/{directory}\"):\n",
    "    image_path = f\"{dir}/{directory}/{files}\"\n",
    "    image = cv2.imread(image_path)\n",
    "    image = img_to_array(image)\n",
    "    image_list.append(image)\n",
    "    label_list.append(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lxIjQIrN8RIM"
   },
   "source": [
    "Next we will read all the images and convert it into array and appending the list created above with the image and its label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_xg0vsmR0Mb2"
   },
   "outputs": [],
   "source": [
    "# Visualize the number of classes count\n",
    "label_counts = pd.DataFrame(label_list).value_counts()\n",
    "label_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6uUXeHnO8rk7"
   },
   "source": [
    "Check for class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tnZwm5tI4187"
   },
   "outputs": [],
   "source": [
    "# Storing number of classes\n",
    "num_classes = len(label_counts)\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5dVFPt7x8zPc"
   },
   "source": [
    "Next we will find out the number of classes that we will be working on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tI7cyZNF2Ieq"
   },
   "outputs": [],
   "source": [
    "# Checking input image shape\n",
    "image_list[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R-Z8eOEz872B"
   },
   "source": [
    "Checking the size of the single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iKE7qOJs2MQm"
   },
   "outputs": [],
   "source": [
    "# Checking labels shape \n",
    "label_list = np.array(label_list)\n",
    "label_list.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2tbAEgM99CeW"
   },
   "source": [
    "Checking the shape of image labels which will be equal to the number of images we are going to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WtG0rEls2OuD"
   },
   "outputs": [],
   "source": [
    "# Splitting dataset\n",
    "x_train, x_test, y_train, y_test = train_test_split(image_list, label_list, test_size=0.2, random_state = 10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJAoH6yH9W2m"
   },
   "source": [
    "Now we will split the data into training and testing using train_test_split() of sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IbuIoPIq2Rvn"
   },
   "outputs": [],
   "source": [
    "# Normalize and reshape data\n",
    "x_train = np.array(x_train, dtype=np.float16) / 225.0\n",
    "x_test = np.array(x_test, dtype=np.float16) / 225.0\n",
    "x_train = x_train.reshape( -1, 224,224,3)\n",
    "x_test = x_test.reshape( -1, 224,224,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ab7k7sU59k0q"
   },
   "source": [
    "Next we will normalize the images by dividing them with 255. Also, we will reshape x_train and x_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YWjfQATN3Yqh"
   },
   "outputs": [],
   "source": [
    "# Label binarizing\n",
    "lb = LabelBinarizer()\n",
    "y_train = lb.fit_transform(y_train)\n",
    "y_test = lb.fit_transform(y_test)\n",
    "print(lb.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wtorEYk5-Lls"
   },
   "source": [
    "Next we will use label binarizer to one hot encode our y data. We will also print the sequence of the classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aQ6wz8Gq4qll"
   },
   "outputs": [],
   "source": [
    "# Splitting the training data set into training and validation data sets\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WLWZvaWj-wUA"
   },
   "source": [
    "Now we will split the training data to validation and training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7hEQHzFa3bZk"
   },
   "outputs": [],
   "source": [
    "# Building model architecture\n",
    "model = Sequential()\n",
    "model.add(Conv2D(8, (3, 3), padding=\"same\",input_shape=(224,224,3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "model.add(Conv2D(16, (3, 3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation=\"relu\"))\n",
    "model.add(Dense(num_classes, activation=\"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s7yL1L0x_MST"
   },
   "source": [
    "Next we will create a network architecture for the model. We have used different types of layers according to their features namely Conv_2d (It is used to create a convolutional kernel that is convolved with the input layer to produce the output tensor), max_pooling2d (It is a downsampling technique which takes out the maximum value over the window defined by poolsize), flatten (It flattens the input and creates a 1D output), Dense (Dense layer produce the output as the dot product of input and kernel). In the last layer we will use softmax as the activation function because it is a multi class classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lyI9PKGi4nVf"
   },
   "outputs": [],
   "source": [
    "# Compiling model\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = Adam(0.0005),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "noaauIqK_aJk"
   },
   "source": [
    "While compiling the model we need to set the type of loss which will be Categorical Crossentropy for our model alongwith this we also need to set the optimizer and the metrics respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wZX1byXc4s2H"
   },
   "outputs": [],
   "source": [
    "# Training the model\n",
    "epochs = 50\n",
    "batch_size = 128\n",
    "history = model.fit(x_train, y_train, batch_size = batch_size, epochs = epochs, validation_data = (x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SupKMddf_wxs"
   },
   "source": [
    "Fitting the model with the data and finding out the accuracy at each epoch to see how our model is learning. Now we will train our model on 50 epochs and a batch size of 128. You can try using more number of epochs to increase accuracy but here we can see that the model has already reached a very high accuracy so we don't need to run it for more. During each epochs we can see how the model is performing by viewing the training and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SWigVc5D4vPu"
   },
   "outputs": [],
   "source": [
    "# Saving model\n",
    "model.save(\"/content/drive/My Drive/bird_species.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_U-Y-jpq_6sQ"
   },
   "source": [
    "Now we will save the model in h5 format to use it later for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-IsevhAa5aWP"
   },
   "outputs": [],
   "source": [
    "#Plot the training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(history.history['accuracy'], color='r')\n",
    "plt.plot(history.history['val_accuracy'], color='b')\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['train', 'val'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lHgitDEbAGfL"
   },
   "source": [
    "Next we will plot the accuracy of the model for the training history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zkLfcLZO5eHy"
   },
   "outputs": [],
   "source": [
    "#Plot the loss history\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(history.history['loss'], color='r')\n",
    "plt.plot(history.history['val_loss'], color='b')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['train', 'val'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6UopTb1aAQLR"
   },
   "source": [
    "Here we will plot the loss of the model for the training history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uf_D53oO51E_"
   },
   "outputs": [],
   "source": [
    "# Calculating test accuracy\n",
    "scores = model.evaluate(x_test, y_test)\n",
    "print(f\"Test Accuracy: {scores[1]*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YwmUAtItAbXM"
   },
   "source": [
    "Evaluating the model to know the accuracy of the model on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JVjY4y0d57l3"
   },
   "outputs": [],
   "source": [
    "# Storing predictions\n",
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fmf5tWW0Agqn"
   },
   "source": [
    "Here we are storing prediction on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cvcqNQx559rh"
   },
   "outputs": [],
   "source": [
    "# Plotting image to compare\n",
    "img = array_to_img(x_test[5])\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2RSUASmOAltK"
   },
   "source": [
    "Visualizing one of the image which we will be further used for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hwTi2TXE5_lV"
   },
   "outputs": [],
   "source": [
    "# Finding max value from predition list and comaparing original value vs predicted\n",
    "labels = lb.classes_\n",
    "print(labels)\n",
    "print(\"Originally : \",labels[np.argmax(y_test[5])])\n",
    "print(\"Predicted : \",labels[np.argmax(y_pred[5])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VuStfSn3AynN"
   },
   "source": [
    "Now, we will create list of labels using object of label binarizer. We will print that list and finally we will print out the prediction and the original label of the image we visualized above using argmax(). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aq05e6P_BesK"
   },
   "source": [
    "# Conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RxED27zNBkD5"
   },
   "source": [
    "We started with loading the dataset into google colab using google drive and visualizing the images. Normalizing is an important step when working with any type of dataset. After that we created a CNN Model which is further used for predicting the bird species using the image supplied to model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kpokKOIBlyP"
   },
   "source": [
    "# Scope:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKp5b0DXBp6P"
   },
   "source": [
    "This project can be used for educational purposes to get a better understanding of how to create network architecture for a CNN model. You can further hyper parameter tune this model to reach a higher accuracy. It can be used by bird sanctuaries to identify different types of birds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hg2F4xNNLije"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Bird Species Prediction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
